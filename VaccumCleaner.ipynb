{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c9752b",
   "metadata": {},
   "source": [
    "# Vacuum Cleaner World Simulation with Different Types of Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7613a4ee",
   "metadata": {},
   "source": [
    "### Step 0\n",
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1414d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from environment import Environment\n",
    "\n",
    "class VisualizeAgents:\n",
    "    def __init__(self, env, agent, steps=100):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.steps = steps\n",
    "\n",
    "    def visualize_floor_before(self):\n",
    "        plt.title(str(self.agent) + ' Floor Before')\n",
    "        plt.imshow(self.env.floor, cmap='Blues', interpolation='nearest')\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_agentPath_before(self):\n",
    "        plt.title(str(self.agent) + ' Path Before')\n",
    "        plt.imshow(self.env.agent_has_been, cmap='Blues', interpolation='nearest')\n",
    "        plt.show()\n",
    "    \n",
    "    def visualize_floor_after(self):\n",
    "        plt.title(str(self.agent) + ' Floor After')\n",
    "        plt.imshow(self.env.floor, cmap='Blues', interpolation='nearest')\n",
    "        plt.show()\n",
    "    \n",
    "    def visualize_agentPath_after(self):\n",
    "        plt.title(str(self.agent) + ' Path After')\n",
    "        plt.imshow(self.env.agent_has_been, cmap='Blues', interpolation='nearest')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10ae88b",
   "metadata": {},
   "source": [
    "### Step 01\n",
    "Implement a simulation environment.\n",
    "Must haves : environment must be a 2D grid of size M X N.\n",
    "Environment must be initialized with some dirt randomly on X% of grids.\n",
    "One grid can have more dirt than another grid.\n",
    "Must have a performance measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dc17aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, width, height):\n",
    "        # Initialize the environment with the specified width and height.\n",
    "        # Sets up the floor grid and agent's path grid.\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.floor = np.zeros((height, width))  # Initialize floor grid\n",
    "        self.agent_has_been = np.zeros((height, width))  # Initialize agent's path grid\n",
    "\n",
    "    def add_dirt(self, dirt_percentage):\n",
    "        # Add dirt to the environment randomly based on the given percentage.\n",
    "        total_cells = self.width * self.height\n",
    "        num_dirt_cells = int((dirt_percentage / 100) * total_cells)\n",
    "        dirt_positions = np.random.choice(total_cells, num_dirt_cells, replace=False)\n",
    "\n",
    "        for pos in dirt_positions:\n",
    "            row = pos // self.width\n",
    "            col = pos % self.width\n",
    "            self.floor[row][col] = 1\n",
    "\n",
    "    def is_dirty(self, x, y):\n",
    "        # Check if the specified tile (x, y) is dirty.\n",
    "        return self.floor[y][x] == 1\n",
    "\n",
    "    def get_bounds(self):\n",
    "        # Get the bounds of the environment (width, height).\n",
    "        return (self.width, self.height)\n",
    "\n",
    "    def update_env(self, x, y):\n",
    "        # Update the environment by removing dirt at the specified tile (x, y).\n",
    "        if self.is_dirty(x, y):\n",
    "            self.floor[y][x] = 0\n",
    "\n",
    "    def update_agent_path(self, x, y):\n",
    "        # Record the agent's path by marking the specified tile (x, y) as visited.\n",
    "        self.agent_has_been[y][x] = 2\n",
    "\n",
    "    def get_stats(self):\n",
    "        # Get statistics about the environment, including total dirt\n",
    "        total_dirt = np.sum(self.floor)\n",
    "        return total_dirt\n",
    "\n",
    "    def visualize(self):\n",
    "        # Visualize the environment (floor) and the agent's path.\n",
    "        print(\"Floor:\")\n",
    "        print(self.floor)\n",
    "        print(\"Agent Path:\")\n",
    "        print(self.agent_has_been)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a41a84",
   "metadata": {},
   "source": [
    "### Step 02\n",
    "Implement a randomized agent.\n",
    "Think of this as a malfunctioning vacuum cleaner.\n",
    "Randomized agent will move up, down, left, right or suck irrespective of what it percepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39a4ecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from environment import Environment\n",
    "import matplotlib.pyplot as plt \n",
    "class RandomAgent:\n",
    "\n",
    "    def __init__(self, startX, startY):\n",
    "        self.positionCol = startX\n",
    "        self.positionRow = startY\n",
    "        self.path = [(startX, startY)]\n",
    "        self.dirty_cells_cleaned = [0]\n",
    "        \n",
    "\n",
    "\n",
    "    def action(self, env):\n",
    "        #update agent environment\n",
    "        env.update_agent_path(self.positionCol, self.positionRow)\n",
    "        self.path.append((self.positionCol, self.positionCol))\n",
    "\n",
    "        act = self.randomAction()\n",
    "        match act:\n",
    "            case \"up\"   : self.positionRow -= 1\n",
    "            case \"down\" : self.positionRow += 1\n",
    "            case \"right\": self.positionCol += 1\n",
    "            case \"left\" : self.positionCol -= 1\n",
    "\n",
    "        # stop vacuum if outside the grid\n",
    "        if((self.positionRow < 0) or (self.positionRow >= env.get_bounds()[1])):\n",
    "            return -1\n",
    "        \n",
    "        if((self.positionCol < 0) or (self.positionCol >= env.get_bounds()[0])):\n",
    "            return -1\n",
    "\n",
    "        #check if dirty and update the floor environment\n",
    "        if(env.is_dirty(self.positionCol, self.positionRow)):\n",
    "            env.update_env(self.positionCol, self.positionRow)\n",
    "            self.dirty_cells_cleaned.append(self.dirty_cells_cleaned[-1] + 1)\n",
    "\n",
    "    def randomAction(self):\n",
    "        randomAct = np.random.choice([\"up\", \"down\", \"right\", \"left\"])\n",
    "        return randomAct\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Random Agent'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1903b472",
   "metadata": {},
   "source": [
    "### Step 03\n",
    "Implement a reflex agent.\n",
    "A agent that selects actions on the basis of the current percept.\n",
    "A reflex agent will move up, down, left, right or suck depeding on the current precept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba8881cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from environment import Environment\n",
    "\n",
    "class reflexAgent:\n",
    "    def __init__(self, start_x, start_y):\n",
    "        self.curr_x = start_x\n",
    "        self.curr_y = start_y\n",
    "        self.path = [(start_x, start_y)]\n",
    "        self.dirty_cells_cleaned = [0]\n",
    "    def update_environment(self, env):\n",
    "        env.update_env(self.curr_x, self.curr_y)\n",
    "\n",
    "    def update_agent_path(self, action):\n",
    "        if action == \"UP\":\n",
    "            self.curr_x -= 1\n",
    "        elif action == \"DOWN\":\n",
    "            self.curr_x += 1\n",
    "        elif action == \"LEFT\":\n",
    "            self.curr_y -= 1\n",
    "        elif action == \"RIGHT\":\n",
    "            self.curr_y += 1\n",
    "\n",
    "    def action(self, env):\n",
    "        act = self.__reflex_action(env)\n",
    "        #update the env\n",
    "        if act == \"SUCK\":\n",
    "            self.update_environment(env)\n",
    "            self.dirty_cells_cleaned.append(self.dirty_cells_cleaned[-1] + 1)\n",
    "        else:\n",
    "        #update agent path\n",
    "            self.update_agent_path(act)\n",
    "            self.path.append((self.curr_x, self.curr_y))\n",
    "        \n",
    "    def __reflex_action(self, env):\n",
    "        #add functionality to return a reflex action\n",
    "        R = env.get_bounds()[1]\n",
    "        C = env.get_bounds()[0]\n",
    "        env.update_agent_path(self.curr_x, self.curr_y)\n",
    "\n",
    "        if env.is_dirty(self.curr_x, self.curr_y):\n",
    "            return \"SUCK\"\n",
    "        elif self.curr_x == 0 and self.curr_y == 0:\n",
    "            return random.choice([\"DOWN\", \"RIGHT\"]) \n",
    "        elif self.curr_x == 0 and self.curr_y == C - 1:\n",
    "            return random.choice([\"DOWN\", \"LEFT\"])\n",
    "        elif self.curr_x == R - 1 and self.curr_y == 0:\n",
    "            return random.choice([\"RIGHT\", \"UP\"])\n",
    "        elif self.curr_x == R - 1 and self.curr_y == C - 1:\n",
    "            return random.choice([\"LEFT\", \"UP\"])\n",
    "        elif self.curr_x == 0: \n",
    "            return random.choice([\"DOWN\", \"RIGHT\", \"LEFT\"])\n",
    "        elif self.curr_x == R - 1: \n",
    "            return random.choice([\"RIGHT\", \"LEFT\", \"UP\"])  \n",
    "        elif self.curr_y == 0: \n",
    "            return random.choice([\"DOWN\", \"RIGHT\", \"UP\"])\n",
    "        elif self.curr_y == C - 1: \n",
    "            return random.choice([\"DOWN\", \"LEFT\", \"UP\"])  \n",
    "        else:\n",
    "            return random.choice([\"DOWN\", \"UP\", \"LEFT\", \"RIGHT\"])\n",
    "       \n",
    "    def visualize_agent_movement(self, env):\n",
    "        #add functionality to visualize environment \n",
    "        print(self.curr_x, self.curr_y)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Reflex Agent'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb29cc2c",
   "metadata": {},
   "source": [
    "### Step 04\n",
    "Implement a model based reflex agent.\n",
    "A agent that selects actions on the basis of the percept history.\n",
    "A model based reflex agent will move up, down, left, right or suck depending on the precept history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36499dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import Environment\n",
    "import random\n",
    "\n",
    "class ModelBasedReflexAgent:\n",
    "    def __init__(self, start_x, start_y):\n",
    "        self.location = (start_x, start_y)  \n",
    "        self.history = {}\n",
    "        self.path = [(start_x, start_y)]  # Initialize path with starting location\n",
    "        self.dirty_cells_cleaned = [0]\n",
    "    def sense(self, env):\n",
    "        # Sensing the current location's state and update the agent's model\n",
    "        x, y = self.location\n",
    "        percept = \"Dirty\" if env.is_dirty(x, y) else \"Clean\"\n",
    "        self.history[self.location] = percept\n",
    "    def act(self, env):\n",
    "        # Deciding the action based on the percept history\n",
    "        C, R= env.get_bounds()\n",
    "        x, y = self.location\n",
    "        if self.history.get(self.location) == \"Dirty\":\n",
    "            return \"Suck\"\n",
    "        elif (x, y + 1) in self.history and self.history[(x, y + 1)] == \"Dirty\":\n",
    "            return \"Down\"\n",
    "        elif (x, y - 1) in self.history and self.history[(x, y - 1)] == \"Dirty\":\n",
    "            return \"Up\"\n",
    "        elif (x + 1, y) in self.history and self.history[(x + 1, y)] == \"Dirty\":\n",
    "            return \"Right\"\n",
    "        elif (x - 1, y) in self.history and self.history[(x - 1, y)] == \"Dirty\":\n",
    "            return \"Left\"\n",
    "        else:\n",
    "            # If no dirty percept nearby, moves randomly\n",
    "            possible_moves = []\n",
    "            if y > 0: possible_moves.append(\"Up\")\n",
    "            if y < R-1: possible_moves.append(\"Down\")\n",
    "            if x > 0: possible_moves.append(\"Left\")\n",
    "            if x < C-1: possible_moves.append(\"Right\")\n",
    "            return random.choice(possible_moves)\n",
    "    def move(self, action, env):\n",
    "        # Moving the agent according to the action\n",
    "        C, R= env.get_bounds()\n",
    "        x, y = self.location\n",
    "        if action == \"Suck\":\n",
    "            env.update_env(x, y)\n",
    "            self.dirty_cells_cleaned.append(self.dirty_cells_cleaned[-1] + 1)\n",
    "        elif action == \"Down\" and y < R-1:\n",
    "            self.location = (x, y+1)\n",
    "            env.update_agent_path(x, y+1)\n",
    "        elif action == \"Up\" and y>0:\n",
    "            self.location = (x, y-1)\n",
    "            env.update_agent_path(x, y-1)\n",
    "        elif action == \"Right\" and x<C-1:\n",
    "            self.location = (x+1, y)\n",
    "            env.update_agent_path(x+1, y)\n",
    "        elif action == \"Left\" and x>0:\n",
    "            self.location = (x-1, y)\n",
    "            env.update_agent_path(x-1, y)\n",
    "\n",
    "\n",
    "        # Update path with the new location after any move or action\n",
    "        self.path.append(self.location)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Model Based Reflex Agent'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5792d68a",
   "metadata": {},
   "source": [
    "### Step 05\n",
    "Run simulations.\n",
    "Report average performance of 100 runs, for all three types of agents on grid sizes (5X5), (10 X 10), (100 X 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e34b80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(rows, columns, dirt_percentage):\n",
    "    \"\"\"\n",
    "    The main function to run the simulation of vacuum cleaner agents.\n",
    "    \"\"\"\n",
    "    # Call the functions or instantiate the classes from each Python file\n",
    "\n",
    "\n",
    "\n",
    "#---------------Random Agent average of 100 runs-----------#\n",
    "    afterRandomAgent = []\n",
    "    for ii in range(100):\n",
    "        env=Environment(rows, columns)\n",
    "        env.add_dirt(dirt_percentage)\n",
    "        agent=RandomAgent(2, 2) # Creating an instance of the RandomAgent class\n",
    "\n",
    "        dirty_tiles = env.get_stats()\n",
    "\n",
    "        for i in range(100):\n",
    "            res=agent.action(env)\n",
    "            if (res==-1):\n",
    "                break\n",
    "\n",
    "        afterRandomAgent.append(dirty_tiles - env.get_stats())\n",
    "\n",
    "    print('Average number of Tiles cleaned from Random Agent is: ' + str(sum(afterRandomAgent)/len(afterRandomAgent)))\n",
    "\n",
    "    #plotting one run\n",
    "    env1 = Environment(rows, columns)\n",
    "    env1.add_dirt(dirt_percentage)\n",
    "    agent1=RandomAgent(2, 2)\n",
    "    visualizer1 = VisualizeAgents(env1, agent1)\n",
    "\n",
    "    visualizer1.visualize_floor_before()\n",
    "    visualizer1.visualize_agentPath_before()\n",
    "\n",
    "    for i in range(100):\n",
    "        res=agent1.action(env1)\n",
    "        if (res==-1):\n",
    "            break\n",
    "\n",
    "    visualizer1.visualize_floor_after()\n",
    "    visualizer1.visualize_agentPath_after()\n",
    "\n",
    "\n",
    "#----------------Reflex Agent average of 100 runs--------------#\n",
    "\n",
    "    afterReflexAgent = []\n",
    "    for ii in range(100):\n",
    "        env = Environment(rows, columns)\n",
    "        env.add_dirt(dirt_percentage)\n",
    "\n",
    "        agent=reflexAgent(2, 2) # Creating an instance of the RandomAgent class\n",
    "\n",
    "        dirty_tiles = env.get_stats()\n",
    "\n",
    "        for i in range(100):\n",
    "            res=agent.action(env)\n",
    "\n",
    "        afterReflexAgent.append(dirty_tiles - env.get_stats())\n",
    "\n",
    "    print('Average number of Tiles cleaned from Reflex Agent is: ' + str(sum(afterReflexAgent)/len(afterReflexAgent)))\n",
    "\n",
    "    #plotting one run\n",
    "    env2 = Environment(rows, columns)\n",
    "    env2.add_dirt(dirt_percentage)\n",
    "    agent2=reflexAgent(2, 2)\n",
    "    visualizer2 = VisualizeAgents(env2, agent2)\n",
    "\n",
    "    visualizer2.visualize_floor_before()\n",
    "    visualizer2.visualize_agentPath_before()\n",
    "\n",
    "    for i in range(100):\n",
    "        res=agent2.action(env2)\n",
    "\n",
    "    visualizer2.visualize_floor_after()\n",
    "    visualizer2.visualize_agentPath_after()\n",
    "\n",
    "\n",
    "#-------------Model Based Reflex Agent average of 100 runs-----------#\n",
    "\n",
    "    afterModelReflexAgent = []\n",
    "    for ii in range(100):\n",
    "        env = Environment(rows, columns)\n",
    "        env.add_dirt(dirt_percentage)\n",
    "\n",
    "        agent=ModelBasedReflexAgent(2, 2) # Creating an instance of the RandomAgent class\n",
    "\n",
    "        dirty_tiles = env.get_stats()\n",
    "\n",
    "        for i in range(100):\n",
    "            agent.sense(env)\n",
    "            action=agent.act(env)\n",
    "            agent.move(action, env)\n",
    "\n",
    "        afterModelReflexAgent.append(dirty_tiles - env.get_stats())\n",
    "\n",
    "    print('Average number of Tiles cleaned from Model Reflex Agent is: ' + str(sum(afterModelReflexAgent)/len(afterModelReflexAgent)))\n",
    "\n",
    "    #plotting one run\n",
    "    env3 = Environment(rows, columns)\n",
    "    env3.add_dirt(dirt_percentage)\n",
    "    agent3=ModelBasedReflexAgent(2, 2)\n",
    "    visualizer3 = VisualizeAgents(env3, agent3)\n",
    "\n",
    "    visualizer3.visualize_floor_before()\n",
    "    visualizer3.visualize_agentPath_before()\n",
    "\n",
    "    for i in range(100):\n",
    "        agent3.sense(env3)\n",
    "        action=agent3.act(env3)\n",
    "        agent3.move(action, env3)\n",
    "\n",
    "    visualizer3.visualize_floor_after()\n",
    "    visualizer3.visualize_agentPath_after()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9d73ca6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'update'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m40\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(rows, columns, dirt_percentage)\u001b[0m\n\u001b[1;32m     10\u001b[0m afterRandomAgent \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     env\u001b[38;5;241m=\u001b[39mEnvironment(rows, columns)\n\u001b[1;32m     13\u001b[0m     env\u001b[38;5;241m.\u001b[39madd_dirt(dirt_percentage)\n\u001b[1;32m     14\u001b[0m     agent\u001b[38;5;241m=\u001b[39mRandomAgent(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# Creating an instance of the RandomAgent class\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/environment.py:246\u001b[0m, in \u001b[0;36mEnvironment.__init__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    245\u001b[0m     spec \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 246\u001b[0m spec\u001b[38;5;241m.\u001b[39mupdate(kw)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspec \u001b[38;5;241m=\u001b[39m spec\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m spec\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'update'"
     ]
    }
   ],
   "source": [
    "main(10, 10, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2023d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a94103b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240b969d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38bbcab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
